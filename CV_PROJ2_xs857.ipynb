{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "mean:0.108707, std_dev0.126335, path:train_data/train_positive/person_and_bike_026a.bmp\n",
      "--1--\n",
      "mean:0.110000, std_dev0.125211, path:train_data/train_positive/crop001070a.bmp\n",
      "--2--\n",
      "mean:0.142157, std_dev0.087001, path:train_data/train_positive/crop001030c.bmp\n",
      "--3--\n",
      "mean:0.116470, std_dev0.119216, path:train_data/train_positive/person_and_bike_151a.bmp\n",
      "--4--\n",
      "mean:0.118138, std_dev0.117564, path:train_data/train_positive/crop001500b.bmp\n",
      "--5--\n",
      "mean:0.124809, std_dev0.110456, path:train_data/train_positive/crop001672b.bmp\n",
      "--6--\n",
      "mean:0.123449, std_dev0.111973, path:train_data/train_positive/crop001275b.bmp\n",
      "--7--\n",
      "mean:0.136821, std_dev0.095172, path:train_data/train_positive/crop001034b.bmp\n",
      "--8--\n",
      "mean:0.101647, std_dev0.132082, path:train_data/train_positive/crop001063b.bmp\n",
      "--9--\n",
      "mean:0.125443, std_dev0.109735, path:train_data/train_positive/crop001278a.bmp\n",
      "--10--\n",
      "train_data/train_negative/no_person__no_bike_264_cut.bmp mean:0.109395, std_dev0.125740, path:train_data/train_negative/no_person__no_bike_264_cut.bmp\n",
      "--11--\n",
      "train_data/train_negative/no_person__no_bike_258_Cut.bmp mean:0.113345, std_dev0.122191, path:train_data/train_negative/no_person__no_bike_258_Cut.bmp\n",
      "--12--\n",
      "train_data/train_negative/01-03e_cut.bmp mean:0.124153, std_dev0.111193, path:train_data/train_negative/01-03e_cut.bmp\n",
      "--13--\n",
      "train_data/train_negative/00000118a_cut.bmp mean:0.114017, std_dev0.121565, path:train_data/train_negative/00000118a_cut.bmp\n",
      "--14--\n",
      "train_data/train_negative/no_person__no_bike_259_cut.bmp mean:0.126534, std_dev0.108475, path:train_data/train_negative/no_person__no_bike_259_cut.bmp\n",
      "--15--\n",
      "train_data/train_negative/00000057a_cut.bmp mean:0.113981, std_dev0.118834, path:train_data/train_negative/00000057a_cut.bmp\n",
      "--16--\n",
      "train_data/train_negative/00000003a_cut.bmp mean:0.120941, std_dev0.114678, path:train_data/train_negative/00000003a_cut.bmp\n",
      "--17--\n",
      "train_data/train_negative/00000090a_cut.bmp mean:0.094648, std_dev0.126604, path:train_data/train_negative/00000090a_cut.bmp\n",
      "--18--\n",
      "train_data/train_negative/no_person__no_bike_219_cut.bmp mean:0.127725, std_dev0.107071, path:train_data/train_negative/no_person__no_bike_219_cut.bmp\n",
      "--19--\n",
      "train_data/train_negative/00000091a_cut.bmp mean:0.115477, std_dev0.120178, path:train_data/train_negative/00000091a_cut.bmp\n",
      "--0--\n",
      "mean:0.108373, std_dev0.126622, path:train_data/test_positive/crop001047b.bmp\n",
      "--1--\n",
      "mean:0.120568, std_dev0.115070, path:train_data/test_positive/crop_000010b.bmp\n",
      "--2--\n",
      "mean:0.102678, std_dev0.131282, path:train_data/test_positive/crop001045b.bmp\n",
      "--3--\n",
      "mean:0.092388, std_dev0.138716, path:train_data/test_positive/crop001028a.bmp\n",
      "--4--\n",
      "mean:0.126728, std_dev0.108249, path:train_data/test_positive/crop001008b.bmp\n",
      "--5--\n",
      "mean:0.089883, std_dev0.130540, path:train_data/test_negative/00000093a_cut.bmp\n",
      "--6--\n",
      "mean:0.119036, std_dev0.116655, path:train_data/test_negative/00000053a_cut.bmp\n",
      "--7--\n",
      "mean:0.119680, std_dev0.115993, path:train_data/test_negative/00000062a_cut.bmp\n",
      "--8--\n",
      "mean:0.144874, std_dev0.082398, path:train_data/test_negative/no_person__no_bike_213_cut.bmp\n",
      "--9--\n",
      "mean:0.132161, std_dev0.101545, path:train_data/test_negative/no_person__no_bike_247_cut.bmp\n",
      "target 1 output 0.27667044794198226\n",
      "Error to output -0.7233295520580177\n",
      "derivate of activation function 0.20012391117756514\n",
      "deltas -0.14475553902816674\n",
      "target 1 output 0.27667044794198226\n",
      "Error to output -0.7233295520580177\n",
      "derivate of activation function 0.20012391117756514\n",
      "target 1 output 0.9999950975070442\n",
      "Error to output -4.902492955816129e-06\n",
      "derivate of activation function 4.902468921378947e-06\n",
      "deltas -2.4034319353167782e-11\n",
      "target 1 output 0.9999950975070442\n",
      "Error to output -4.902492955816129e-06\n",
      "derivate of activation function 4.902468921378947e-06\n",
      "target 1 output 0.9999998136161355\n",
      "Error to output -1.8638386445335442e-07\n",
      "derivate of activation function 1.863838297144095e-07\n",
      "deltas -3.473893845378759e-14\n",
      "target 1 output 0.9999998136161355\n",
      "Error to output -1.8638386445335442e-07\n",
      "derivate of activation function 1.863838297144095e-07\n",
      "target 1 output 0.9999999213489119\n",
      "Error to output -7.865108808680077e-08\n",
      "derivate of activation function 7.865108190080711e-08\n",
      "deltas -6.1859931707025615e-15\n",
      "target 1 output 0.9999999213489119\n",
      "Error to output -7.865108808680077e-08\n",
      "derivate of activation function 7.865108190080711e-08\n",
      "target 1 output 0.9999999713364779\n",
      "Error to output -2.8663522111394002e-08\n",
      "derivate of activation function 2.86635212897965e-08\n",
      "deltas -8.215974762804947e-16\n",
      "target 1 output 0.9999999713364779\n",
      "Error to output -2.8663522111394002e-08\n",
      "derivate of activation function 2.86635212897965e-08\n",
      "target 1 output 0.9999998989635401\n",
      "Error to output -1.0103645986525578e-07\n",
      "derivate of activation function 1.0103644965688956e-07\n",
      "deltas -1.0208365190686258e-14\n",
      "target 1 output 0.9999998989635401\n",
      "Error to output -1.0103645986525578e-07\n",
      "derivate of activation function 1.0103644965688956e-07\n",
      "target 1 output 0.9999982911552512\n",
      "Error to output -1.7088447488156078e-06\n",
      "derivate of activation function 1.7088418286652323e-06\n",
      "deltas -2.9201453854710427e-12\n",
      "target 1 output 0.9999982911552512\n",
      "Error to output -1.7088447488156078e-06\n",
      "derivate of activation function 1.7088418286652323e-06\n",
      "target 1 output 0.9999993341098381\n",
      "Error to output -6.658901618994406e-07\n",
      "derivate of activation function 6.658897184897329e-07\n",
      "deltas -4.4340941245230114e-13\n",
      "target 1 output 0.9999993341098381\n",
      "Error to output -6.658901618994406e-07\n",
      "derivate of activation function 6.658897184897329e-07\n",
      "target 1 output 0.999999910014952\n",
      "Error to output -8.998504796853268e-08\n",
      "derivate of activation function 8.998503987122383e-08\n",
      "deltas -8.097308129262403e-15\n",
      "target 1 output 0.999999910014952\n",
      "Error to output -8.998504796853268e-08\n",
      "derivate of activation function 8.998503987122383e-08\n",
      "target 1 output 0.9999991000288847\n",
      "Error to output -8.999711152979373e-07\n",
      "derivate of activation function 8.999703053499289e-07\n",
      "deltas -8.099472794408007e-13\n",
      "target 1 output 0.9999991000288847\n",
      "Error to output -8.999711152979373e-07\n",
      "derivate of activation function 8.999703053499289e-07\n",
      "target 0 output 0.999999478894279\n",
      "Error to output 0.999999478894279\n",
      "derivate of activation function 5.211054494674273e-07\n",
      "deltas 5.211051779163964e-07\n",
      "target 0 output 0.999999478894279\n",
      "Error to output 0.999999478894279\n",
      "derivate of activation function 5.211054494674273e-07\n",
      "target 0 output 0.9999992954633455\n",
      "Error to output 0.9999992954633455\n",
      "derivate of activation function 7.045361580892323e-07\n",
      "deltas 7.045356617176846e-07\n",
      "target 0 output 0.9999992954633455\n",
      "Error to output 0.9999992954633455\n",
      "derivate of activation function 7.045361580892323e-07\n",
      "target 0 output 0.9999984996100307\n",
      "Error to output 0.9999984996100307\n",
      "derivate of activation function 1.5003877180966534e-06\n",
      "deltas 1.500385466929971e-06\n",
      "target 0 output 0.9999984996100307\n",
      "Error to output 0.9999984996100307\n",
      "derivate of activation function 1.5003877180966534e-06\n",
      "target 0 output 0.9999273667667009\n",
      "Error to output 0.9999273667667009\n",
      "derivate of activation function 7.262795771251141e-05\n",
      "deltas 7.262268250911484e-05\n",
      "target 0 output 0.9999273667667009\n",
      "Error to output 0.9999273667667009\n",
      "derivate of activation function 7.262795771251141e-05\n",
      "target 0 output 0.9999999615199647\n",
      "Error to output 0.9999999615199647\n",
      "derivate of activation function 3.8480033788581776e-08\n",
      "deltas 3.848003230786872e-08\n",
      "target 0 output 0.9999999615199647\n",
      "Error to output 0.9999999615199647\n",
      "derivate of activation function 3.8480033788581776e-08\n",
      "target 0 output 0.9999998859764289\n",
      "Error to output 0.9999998859764289\n",
      "derivate of activation function 1.1402355813916762e-07\n",
      "deltas 1.1402354513779432e-07\n",
      "target 0 output 0.9999998859764289\n",
      "Error to output 0.9999998859764289\n",
      "derivate of activation function 1.1402355813916762e-07\n",
      "target 0 output 0.9999988176184758\n",
      "Error to output 0.9999988176184758\n",
      "derivate of activation function 1.182380126219033e-06\n",
      "deltas 1.182378728194617e-06\n",
      "target 0 output 0.9999988176184758\n",
      "Error to output 0.9999988176184758\n",
      "derivate of activation function 1.182380126219033e-06\n",
      "target 0 output 0.9999996214485002\n",
      "Error to output 0.9999996214485002\n",
      "derivate of activation function 3.785513565121241e-07\n",
      "deltas 3.7855121321094034e-07\n",
      "target 0 output 0.9999996214485002\n",
      "Error to output 0.9999996214485002\n",
      "derivate of activation function 3.785513565121241e-07\n",
      "target 0 output 0.9999997089219338\n",
      "Error to output 0.9999997089219338\n",
      "derivate of activation function 2.910779814793226e-07\n",
      "deltas 2.9107789675290665e-07\n",
      "target 0 output 0.9999997089219338\n",
      "Error to output 0.9999997089219338\n",
      "derivate of activation function 2.910779814793226e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 0 output 0.9999974978121124\n",
      "Error to output 0.9999974978121124\n",
      "derivate of activation function 2.502181626609961e-06\n",
      "deltas 2.5021753656814027e-06\n",
      "target 0 output 0.9999974978121124\n",
      "Error to output 0.9999974978121124\n",
      "derivate of activation function 2.502181626609961e-06\n",
      "*** 0 4.999919276\n",
      "target [1] prediction 0.9999972032316407\n",
      "target [1] prediction 0.9999990865611105\n",
      "target [1] prediction 0.9999931718082362\n",
      "target [1] prediction 0.9996270524731057\n",
      "target [1] prediction 0.9999939104589988\n",
      "target [0] prediction 0.9999981683739576\n",
      "target [0] prediction 0.9999997758208613\n",
      "target [0] prediction 0.9999977483633674\n",
      "target [0] prediction 0.999999649876284\n",
      "target [0] prediction 0.9999968787982134\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import sys\n",
    "from numpy import arctan2\n",
    "import random\n",
    "import math\n",
    "import glob as gb\n",
    "import matplotlib.pyplot as plt\n",
    "from histogram import gradient, magnitude_orientation, hog, visualise_histogram\n",
    "\n",
    "PREWITT_GX = np.array([[-1,0,1],\n",
    "                      [-1,0,1],\n",
    "                      [-1,0,1]])\n",
    "\n",
    "PREWITT_GY = np.array([[1,1,1],\n",
    "                      [0,0,0],\n",
    "                      [-1,-1,-1]])\n",
    "# \n",
    "CELL_ROW = int(8)\n",
    "CELL_COL = int(8)\n",
    "\n",
    "img_name = \"train_data/train_positive/crop001030c.bmp\"\n",
    "img = cv.imread(img_name, 1)\n",
    "\n",
    "WINDOW_ROW = 160\n",
    "WINDOW_COL = 96\n",
    "\n",
    "# the number of cell in each window\n",
    "\n",
    "# row: 20\n",
    "CELL_ROW_PER_WINDOW = int(WINDOW_ROW / CELL_ROW)\n",
    "# col: 12\n",
    "CELL_COL_PER_WINDOW = int(WINDOW_COL / CELL_COL)\n",
    "\n",
    "BLOCK_ROW = 2\n",
    "BLOCK_COL = 2\n",
    "\n",
    "# the number of block in each window\n",
    "# row: 19\n",
    "BLOCK_ROW_PER_WINDOW = int(CELL_ROW_PER_WINDOW - BLOCK_ROW + 1)\n",
    "# col: 11\n",
    "BLOCK_COL_PER_WINDOW = int(CELL_COL_PER_WINDOW - BLOCK_COL + 1)\n",
    "IMG_ROW = 160\n",
    "IMG_COL = 96\n",
    "\n",
    "# step1: convert color image into gray value\n",
    "def color2gray(img):\n",
    "    gray = 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
    "    gray_img = gray.astype(np.uint8)\n",
    "    return gray_img\n",
    "\n",
    "# step2: \n",
    "# do noramlization\n",
    "def normalize(ndarr):\n",
    "    result = np.zeros([IMG_ROW, IMG_COL])\n",
    "    result = np.abs(ndarr)\n",
    "    max = np.max(result)\n",
    "    \n",
    "    if max > 255:\n",
    "        # take the max value greater than 255, get normalization ratio\n",
    "        ratio = math.ceil(max / 255)\n",
    "        result = np.rint(result / ratio)\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "# return if it is undefined area\n",
    "def isCal(i, j, bound):\n",
    "    if i >= bound and \\\n",
    "    i < IMG_ROW - bound and \\\n",
    "    j >= bound and \\\n",
    "    j < IMG_COL - bound:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# do the convolution\n",
    "# (a) slice the matrix according to the center\n",
    "# (b) np.multiply\n",
    "def conv(i,j,img,kernel):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    kernel_bound = int(kernel_size / 2);\n",
    "    \n",
    "    up = i - kernel_bound\n",
    "    down = i + kernel_bound + 1\n",
    "    left = j - kernel_bound\n",
    "    right = j + kernel_bound + 1\n",
    "    \n",
    "    sliced = img[up:down, left:right]\n",
    "    return int(np.sum(sliced * kernel))\n",
    "\n",
    "# Step2: gradient operator\n",
    "# (a) Prewitt's operator\n",
    "# (b) do the convolution\n",
    "def gradient_operator(gimg):\n",
    "    resultGX = np.zeros([IMG_ROW, IMG_COL])\n",
    "    resultGY = np.zeros([IMG_ROW, IMG_COL])\n",
    "    \n",
    "    kernel_size = PREWITT_GX.shape[0]\n",
    "    kernel_bound = int(kernel_size / 2);\n",
    "    \n",
    "    # ******\n",
    "    bound = kernel_bound\n",
    "    count = 0\n",
    "    for i in range(IMG_ROW):\n",
    "        for j in range(IMG_COL):\n",
    "            if isCal(i, j, bound):\n",
    "                resultGX[i,j] = conv(i,j,gimg, PREWITT_GX)\n",
    "                resultGY[i,j] = conv(i,j,gimg, PREWITT_GY)\n",
    "            else:\n",
    "                resultGX[i,j] = 0\n",
    "                resultGY[i,j] = 0\n",
    "    return resultGX, resultGY\n",
    "\n",
    "# calculate the magnitude\n",
    "def magnitude(resultGX,resultGY):\n",
    "    resultMG = np.zeros([IMG_ROW, IMG_COL])\n",
    "    for i in range(IMG_ROW):\n",
    "        for j in range(IMG_COL):\n",
    "            resultMG[i,j] = math.sqrt(math.pow(resultGX[i,j],2)+ \\\n",
    "                                      math.pow(resultGY[i,j],2))\n",
    "    return resultMG\n",
    "\n",
    "\n",
    "# ******HOG********\n",
    "\n",
    "# unsigned the magnitude angle\n",
    "def get_orientation(Gx, Gy):\n",
    "    return np.abs((arctan2(Gy, Gx) * 180 / np.pi))\n",
    "\n",
    "# get histogram for the cell\n",
    "def get_histogram(magnitude_slice, orientation_slice):\n",
    "    hist = np.zeros(9,dtype = np.float);\n",
    "    for i in range(CELL_ROW):\n",
    "        for j in range(CELL_COL):\n",
    "            # 10/20 0.5 left 0 right 1\n",
    "            divide_res = orientation_slice[i,j] / 20\n",
    "            left_bin_num = (math.floor(divide_res)) % 9\n",
    "            right_bin_num = (math.ceil(divide_res)) % 9\n",
    "            \n",
    "            left_bin_ratio = (orientation_slice[i,j] - left_bin_num * 20) / 20\n",
    "            right_bin_ratio = 1 - left_bin_ratio\n",
    "            \n",
    "            hist[left_bin_num] += magnitude_slice[i,j] * left_bin_ratio\n",
    "            hist[right_bin_num] += magnitude_slice[i,j] * right_bin_ratio\n",
    "    return hist\n",
    "\n",
    "# get the window, and calculate the cell inside, get 20 * 12 * 9\n",
    "def get_window_cell(window_magnitude, window_orientation):\n",
    "    window_cell = np.zeros([CELL_ROW_PER_WINDOW,CELL_COL_PER_WINDOW,9], dtype = np.float)\n",
    "    \n",
    "    for i in range(CELL_ROW_PER_WINDOW):\n",
    "        for j in range(CELL_COL_PER_WINDOW):\n",
    "            up = i * CELL_ROW\n",
    "            down = up + CELL_ROW\n",
    "            left = j * CELL_COL\n",
    "            right = left + CELL_COL\n",
    "            window_cell[i,j] = get_histogram(window_magnitude[up:down,left:right], \\\n",
    "                                             window_orientation[up:down,left:right])\n",
    "    return window_cell\n",
    "\n",
    "# normalize over block\n",
    "def L2_norm(block):\n",
    "    norm = np.sqrt(np.sum(np.square(block)))\n",
    "    if norm == 0:\n",
    "        return block\n",
    "    return block / norm\n",
    "\n",
    "def normalize_over_block(window_cell):\n",
    "    final_descriptor = np.zeros([BLOCK_ROW_PER_WINDOW,BLOCK_COL_PER_WINDOW,36], dtype = np.float)\n",
    "    for i in range(BLOCK_ROW_PER_WINDOW):\n",
    "        for j in range(BLOCK_COL_PER_WINDOW):\n",
    "            up = i\n",
    "            down = i + BLOCK_ROW\n",
    "            left = j\n",
    "            right = j + BLOCK_COL\n",
    "            block = window_cell[up:down, left:right].flatten()\n",
    "            final_descriptor[i,j] = L2_norm(block)\n",
    "    return final_descriptor.flatten().tolist()\n",
    "\n",
    "def get_descriptor(img_path):\n",
    "    image = cv.imread(img_path, 1)\n",
    "    gray_image = color2gray(image)\n",
    "    Gx, Gy = gradient_operator(gray_image)\n",
    "    mag_img = magnitude(Gx, Gy)\n",
    "    orientation_img = get_orientation(Gx, Gy)\n",
    "    window_cell = get_window_cell(mag_img, orientation_img)\n",
    "    final_descriptor = normalize_over_block(window_cell)\n",
    "    return final_descriptor\n",
    "    \n",
    "def get_trainning_set():\n",
    "    positive_img_path = gb.glob(\"train_data/train_positive/*.bmp\")\n",
    "    train_sets = []\n",
    "    count_p = 0\n",
    "    for path in positive_img_path:\n",
    "        sub_train_set = []\n",
    "        train_pos_des = get_descriptor(path)\n",
    "        sub_train_set.append(train_pos_des)\n",
    "        print(\"--{0:d}--\".format(count_p))\n",
    "        print(\"mean:{0:f}, std_dev{1:f}, path:{2:s}\".format(np.mean(np.array(train_pos_des)), np.std(np.array(train_pos_des)), path))\n",
    "        sub_train_set.append([1])\n",
    "        train_sets.append(sub_train_set)\n",
    "        count_p += 1\n",
    "        \n",
    "    negative_img_path = gb.glob(\"train_data/train_negative/*.bmp\")\n",
    "    for path in negative_img_path:\n",
    "        sub_train_set = []\n",
    "        train_neg_des = get_descriptor(path)\n",
    "        print(\"--{0:d}--\".format(count_p))\n",
    "        print(path,\"mean:{0:f}, std_dev{1:f}, path:{2:s}\".format(np.mean(np.array(train_neg_des)), np.std(np.array(train_neg_des)),path))\n",
    "        sub_train_set.append(train_neg_des)\n",
    "        sub_train_set.append([0])\n",
    "        train_sets.append(sub_train_set)\n",
    "        count_p += 1\n",
    "        \n",
    "    return train_sets\n",
    "\n",
    "def get_test_set():\n",
    "    positive_img_path = gb.glob(\"train_data/test_positive/*.bmp\")\n",
    "    test_sets = []\n",
    "    count_p = 0\n",
    "    for path in positive_img_path:\n",
    "        sub_train_set = []\n",
    "        train_pos_des = get_descriptor(path)\n",
    "        sub_train_set.append(train_pos_des)\n",
    "        print(\"--{0:d}--\".format(count_p))\n",
    "        print(\"mean:{0:f}, std_dev{1:f}, path:{2:s}\".format(np.mean(np.array(train_pos_des)), np.std(np.array(train_pos_des)), path))\n",
    "        sub_train_set.append([1])\n",
    "        test_sets.append(sub_train_set)\n",
    "        count_p += 1\n",
    "        \n",
    "    negative_img_path = gb.glob(\"train_data/test_negative/*.bmp\")\n",
    "    for path in negative_img_path:\n",
    "        sub_train_set = []\n",
    "        train_neg_des = get_descriptor(path)\n",
    "        print(\"--{0:d}--\".format(count_p))\n",
    "        print(\"mean:{0:f}, std_dev{1:f}, path:{2:s}\".format(np.mean(np.array(train_neg_des)), np.std(np.array(train_neg_des)),path))\n",
    "        sub_train_set.append(train_neg_des)\n",
    "        sub_train_set.append([0])\n",
    "        test_sets.append(sub_train_set)\n",
    "        count_p += 1\n",
    "    return test_sets\n",
    "# neural network\n",
    "# # nn: 50 learning rate: 0.1 output weight: 0.1\n",
    "\n",
    "#\n",
    "# Shorthand:\n",
    "#   \"pd_\" as a variable prefix means \"partial derivative\"\n",
    "#   \"d_\" as a variable prefix means \"derivative\"\n",
    "#   \"_wrt_\" is shorthand for \"with respect to\"\n",
    "#   \"w_ho\" and \"w_ih\" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively\n",
    "#\n",
    "# Comment references:\n",
    "#\n",
    "# [1] Wikipedia article on Backpropagation\n",
    "#   http://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error\n",
    "# [2] Neural Networks for Machine Learning course on Coursera by Geoffrey Hinton\n",
    "#   https://class.coursera.org/neuralnets-2012-001/lecture/39\n",
    "# [3] The Back Propagation Algorithm\n",
    "#   https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf\n",
    "\"\"\"\n",
    "init:\n",
    "    num_inputs:\n",
    "    num_hidden:\n",
    "    num_outputs:\n",
    "    hidden_layer_weights:\n",
    "    output_layer_weights\n",
    "    \n",
    "attribute:\n",
    "    learning rate\n",
    "    \n",
    "method:\n",
    "    feed_forward: inputs->feed_forward, get result sends to ouput layer, return the result\n",
    "    train(self, training_inputs, training_outputs)\n",
    "\"\"\"\n",
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.1\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, output_layer_weights = None,  hidden_layer_bias=None, output_layer_bias=None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, 0,hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, 1,output_layer_bias)\n",
    "\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random()*random.randint(-1,1))\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "        \n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(0.1 * random.random()*random.randint(-1,1))\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        #self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Uses online learning, ie updating the weights after each training case\n",
    "   \n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "\n",
    "        # 1. Output neuron deltas\n",
    "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "\n",
    "            # ∂E/∂zⱼ\n",
    "            print(\"deltas\", self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o]))\n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "            \n",
    "        # 2. Hidden neuron deltas\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "\n",
    "            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "            d_error_wrt_hidden_neuron_output = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "\n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "        \n",
    "        # 3. Update output neuron weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "                \n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "        # 4. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "                \n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "        return total_error\n",
    "    \n",
    "    def test(self, test_sets):\n",
    "        for t in range(len(test_sets)):\n",
    "            training_inputs, training_outputs = test_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            print(\"target\", training_outputs, \"prediction\", self.output_layer.neurons[0].output)\n",
    "        f = open(\"demofile.txt\", \"a\")\n",
    "        o_w = self.output_layer.neurons[0].weights\n",
    "        h_w = []\n",
    "        for i in range(self.num_hidden):\n",
    "            h_w.append(self.hidden_layer.neurons[i].weights)\n",
    "        f.write(\"********output**************\")\n",
    "        f.write(''.join(str(e) for e in o_w))\n",
    "        f.write(\"********input**************\")\n",
    "        f.write(''.join(str(e) for e in h_w))\n",
    "        \n",
    "'''\n",
    "init:\n",
    "    (a) num_neurons\n",
    "    (b) choose whether it is hidden or output\n",
    "    \n",
    "attributes:\n",
    "    neurons(list):\n",
    "    \n",
    "method:\n",
    "    feed_forward(self, inputs): set output of every neuron\n",
    "'''\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, hidden_0_output_1, bias):\n",
    "        self.neurons = []\n",
    "        \n",
    "        self.bias = bias if bias else random.randint(-1,1) * random.random()\n",
    "        if hidden_0_output_1 == 0:\n",
    "            for i in range(num_neurons):\n",
    "                self.neurons.append(Hidden_Neuron(self.bias))\n",
    "        else:\n",
    "            for i in range(num_neurons):\n",
    "                self.neurons.append(Output_Neuron(self.bias))\n",
    "\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "'''\n",
    "procedure:\n",
    "    inputs -> |net input| neuron(squash) -> output -> error\n",
    "\n",
    "attribute:\n",
    "    weights(list): previous weights\n",
    "    inputs(list): \n",
    "    output(value): \n",
    "    \n",
    "method:\n",
    "    calculate_output: assign the value to the output\n",
    "    calculate_total_net_input(self):\n",
    "    squash: (a)hidden layer: ReLU (b)output layer: sigmoid\n",
    "    \n",
    "    calculate_pd_error_wrt_total_net_input(self, target_output): (a) * (b)\n",
    "    (a) calculate_pd_error_wrt_output(target_output): ∂Error / ∂Output\n",
    "    (b) calculate_pd_total_net_input_wrt_input(self): ∂Output / ∂Netinput\n",
    "    (c) calculate_pd_total_net_input_wrt_weight(self, index): ∂Netinput / ∂wi\n",
    "'''\n",
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        print(\"target\", target_output, \"output\",self.output)\n",
    "        print(\"Error to output\", self.calculate_pd_error_wrt_output(target_output))\n",
    "        print(\"derivate of activation function\", self.calculate_pd_total_net_input_wrt_input())\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5 * ((target_output - self.output)**2)\n",
    "\n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "class Hidden_Neuron(Neuron):\n",
    "    def squash(self, total_net_input):\n",
    "        return max(0, total_net_input)\n",
    "    \n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        if self.output > 0:\n",
    "            return self.output\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "class Output_Neuron(Neuron):\n",
    "    def squash(self, total_net_input):\n",
    "        try:\n",
    "            return 1 / (1 + math.exp(-total_net_input))\n",
    "        except:\n",
    "            print(\"ERROR\", total_net_input)\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "# main\n",
    "\n",
    "# pos_train = get_descriptor(\"train_data/train_positive/crop001030c.bmp\")\n",
    "# neg_train = get_descriptor(\"train_data/train_negative/00000091a_cut.bmp\")\n",
    "# training_sets = [[pos_train,[1]], [neg_train,[0]]]\n",
    "\n",
    "# test_pos_des = get_descriptor(\"train_data/test_positive/crop001008b.bmp\")\n",
    "# test_neg_des = get_descriptor(\"train_data/train_negative/01-03e_cut.bmp\")\n",
    "# test_set1 = [[test_pos_des,[1]],[test_neg_des,[0]]]\n",
    "print\n",
    "\n",
    "training_sets = get_trainning_set()\n",
    "test_sets = get_test_set()\n",
    "nn = NeuralNetwork(len(training_sets[0][0]), 50, len(training_sets[0][1]))\n",
    "for i in range(1):\n",
    "    for j in range(20):\n",
    "        training_inputs, training_outputs = training_sets[j]\n",
    "        nn.train(training_inputs, training_outputs)\n",
    "    print(\"***\",i, round(nn.calculate_total_error(training_sets), 9))\n",
    "nn.test(test_sets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gray_img = color2gray(img)\n",
    "# h = hog(gray_img, cell_size=(8, 8), cells_per_block=(2, 2), visualise=False, nbins=9, signed_orientation=False, normalise=True)\n",
    "# im2 = visualise_histogram(h, 8, 8, False)\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "# get magnitude \n",
    "\n",
    "# print(get_orientation(Gx,Gy))\n",
    "\n",
    "# hog = cv.HOGDescriptor()\n",
    "# h = hog.compute(gray_img)\n",
    "# print(h)\n",
    "\n",
    "# cv.namedWindow(\"comparsion\",1)\n",
    "# cv.imshow(\"original\", img)\n",
    "# cv.imshow(\"gray\", gray_img)\n",
    "# cv.imshow(\"im2\",im2)\n",
    "# cv.imshow(\"myhog\",np.array(training_sets[0][0]))\n",
    "# cv.imshow(\"Gx\", normalize(Gx))\n",
    "# cv.imshow(\"Gy\", normalize(Gy))\n",
    "# cv.imshow(\"norm_mag\", norm_mag_img)\n",
    "\n",
    "# key = cv.waitKey(0)\n",
    "# if key == 27:\n",
    "#     cv.destroyAllWindows()\n",
    "#     cv.waitKey(1)\n",
    "#     cv.waitKey(1)\n",
    "#     cv.waitKey(1)\n",
    "#     cv.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
